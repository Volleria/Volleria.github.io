<!DOCTYPE html>
<html lang="zh-cmn-Hans">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>暗通道去雾及两种超分算法的简单对比 | Volleria 个人博客</title>
  
    <link rel="icon" href="/assets/rubbish.png">
  
  
  
  <!--link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css"-->
  
<link rel="stylesheet" href="//cdn.jsdelivr.net/highlight.js/9.10.0/styles/github-gist.min.css">

  
<link rel="stylesheet" href="/css/style.css">

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"><link rel="alternate" href="/atom.xml" title="Volleria 个人博客" type="application/atom+xml">
</head>

<body>
<div class="Shell">
    <aside class='SideBar'>
    <section class='avatar' style="background-image: url(/assets/back222.png)">
        <div class='av-pic' style="background-image: url(/assets/header.png)">
        </div>
    </section>
    <section class='menu'>
        <div>Volleria 个人博客</div>
        
        <ul>
          
            <a href="/" class="Btn">
              <li>Home</li>
            </a>  
          
            <a href="/archives/" class="Btn">
              <li>Archive</li>
            </a>  
          
            <a href="/tags/" class="Btn">
              <li>Tags</li>
            </a>  
          
            <a href="/categories/" class="Btn">
              <li>Categories</li>
            </a>  
          
            <a href="/about/" class="Btn">
              <li>About</li>
            </a>  
          
        </ul>
    </section>
    <section class="media">
        
            
                <a target="_blank" rel="noopener" href="https://github.com/Volleria">
                    <img src="/assets/GitHub.svg" />
                </a>
            
        
            
                <a href="mailto:EKKO0908@outlook.com">
                    <img src="/assets/E-Mail.svg" />
                </a>
            
        
    </section>
</aside>

    <div class="container">
        <div data-pager-shell>
            <div>
  <article class='ContentView'>
    <header class='PageTitle'>
        <h1>暗通道去雾及两种超分算法的简单对比</h1>
    </header>

    <section>
      <h2 id="Ⅰ、暗通道先验图像去雾霾"><a href="#Ⅰ、暗通道先验图像去雾霾" class="headerlink" title="Ⅰ、暗通道先验图像去雾霾"></a>Ⅰ、暗通道先验图像去雾霾</h2><p>参考论文：K. He, J. Sun and X. Tang, “Single Image Haze Removal Using Dark Channel Prior,” in IEEE  Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 12, pp. 2341-2353,  Dec. 2011.</p>
<p>阅读地址：<a target="_blank" rel="noopener" href="https://readpaper.com/pdf-annotate/note?noteId=696733953373192192&amp;pdfId=4531190175211610113">https://readpaper.com/pdf-annotate/note?noteId=696733953373192192&amp;pdfId=4531190175211610113</a></p>
<span id="more"></span>

<h3 id="一、论文简介"><a href="#一、论文简介" class="headerlink" title="一、论文简介"></a>一、论文简介</h3><p>在本文中，作者提出了一种简单而有效的图像先验-暗通道，用于去除单个输入图像中的雾霾。暗通道先验是一种对室外无雾图像的统计。它是基于一个关键的观察-在室外无雾图像中的大多数局部斑块包含一些像素，其强度在至少一个彩色通道非常低。利用这一先验和烟雾成像模型，我们可以直接估计雾霾的厚度，并恢复一幅高质量的无雾霾图像。在各种模糊图像上的结果表明了所提出的先验方法的有效性。</p>
<h3 id="二、具体过程介绍"><a href="#二、具体过程介绍" class="headerlink" title="二、具体过程介绍"></a>二、具体过程介绍</h3><p>在计算机视觉领域，存在一个对于雾霾图像的模型定义：<br>$$<br>I(x)&#x3D; J(x)t(x) + A(1-t(x))<br>$$<br>其中</p>
<p>$I(x)$ 为我们实际可以观测到的亮度，也可以理解为带雾的原图，是一个已知的值</p>
<p>$J(x)$ 可以理解为去雾之后的结果图像</p>
<p>$t(x)$ 为透射率（transmission from scene to camera）</p>
<p>$A$ 为全球大气光值（atmospheric light）</p>
<p><strong>最终目标</strong>：从 $I$ 中恢复出 $J$ 、$A$ 和 $t$  </p>
<h5 id="1、暗通道图像"><a href="#1、暗通道图像" class="headerlink" title="1、暗通道图像"></a>1、暗通道图像</h5><p>根据原文中的公式获取暗通道图像并进行腐蚀操作<br>$$<br>J^{dark}(x) &#x3D; \underset {c\in {r,g,b }}{min}  \ \underset {y \in \Omega(x) }{(\ min}\ (J^c(y)))<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">DarkChannel</span>(<span class="params">image, size</span>):</span><br><span class="line">    b, g, r = cv2.split(image)  <span class="comment"># 顺序是b,g,r，不是r,g,b</span></span><br><span class="line">    min_channel = cv2.<span class="built_in">min</span>(cv2.<span class="built_in">min</span>(r, g), b)  <span class="comment"># 得到三通道中的最小值</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># cv2.getStructuringElement()</span></span><br><span class="line">    <span class="comment"># 这个函数的第一个参数表示内核的形状，有三种形状可以选择。</span></span><br><span class="line">    <span class="comment"># 腐蚀操作可以看做 将图像（或图像的一部分区域，我们称之为A）与核进行卷积</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 腐蚀（erosion）</span></span><br><span class="line">    <span class="comment"># 腐蚀就是原图中的高亮部分被腐蚀，效果图拥有比原图更小的高亮区域</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 膨胀和腐蚀的主要用途:</span></span><br><span class="line">    <span class="comment"># 1、消除噪声</span></span><br><span class="line">    <span class="comment"># 2、分割出独立的图像元素，在图像中连接相邻的元素</span></span><br><span class="line">    <span class="comment"># 3、寻找图像中明显的极大值或极小值区</span></span><br><span class="line">    <span class="comment"># 4、求出图像的梯度</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 使用滤波窗口大小为15*15</span></span><br><span class="line">    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (size, size))</span><br><span class="line">    dark_channel = cv2.erode(min_channel, kernel)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dark_channel</span><br></pre></td></tr></table></figure>



<p>腐蚀前：</p>
<img src="https://s3.bmp.ovh/imgs/2022/06/15/fe68929e1cd6270c.jpg" style="zoom:67%;" />

<p>腐蚀后：</p>
<img src="https://s3.bmp.ovh/imgs/2022/06/15/b1e4bbdb4fe5aae0.jpg" style="zoom:67%;" />





<h5 id="2、大气光值"><a href="#2、大气光值" class="headerlink" title="2、大气光值"></a>2、大气光值</h5><p>由论文可知，如果有一个图像中，存在一个无限远的距离的像素存在，这个时候，此像素的透射率几乎为0。这个图像中最亮最亮的值所在的像素可以被看做是雾遮盖的程度最大也是其值可以看做是几乎等同于 $A$ 。 </p>
<p>我们可以从暗通道图中求取大气光值 $A$</p>
<ul>
<li>在暗通道图中按照亮度的大小提取最亮的前0.1%的像素</li>
<li>在原始雾图I(x)中找对应位置上具有最高亮度的点的值，此处我们通过累加求平均值获得相对稳定值</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Atmospheric_light</span>(<span class="params">image, dark_channel</span>):</span><br><span class="line">    image_size = image.shape[<span class="number">0</span>] * image.shape[<span class="number">1</span>]  <span class="comment"># 总像素数量</span></span><br><span class="line">    pixel_num = <span class="built_in">int</span>(<span class="built_in">max</span>(math.floor(image_size / <span class="number">1000</span>), <span class="number">1</span>))  <span class="comment"># 提取的像素数量</span></span><br><span class="line"></span><br><span class="line">    dark_vec = dark_channel.reshape(image_size)  <span class="comment"># 将暗通道图的值转为一维</span></span><br><span class="line">    image_vec = image.reshape(image_size, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    indices = dark_vec.argsort()[-pixel_num:]  <span class="comment"># 前0.1%的像素下标</span></span><br><span class="line"></span><br><span class="line">    atmosphere_max = np.zeros([<span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 求取平均值</span></span><br><span class="line">    <span class="keyword">for</span> ind <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, pixel_num):</span><br><span class="line">        atmosphere_max += image_vec[indices[ind]]</span><br><span class="line">    A = atmosphere_max / pixel_num</span><br><span class="line">    <span class="keyword">return</span> A</span><br></pre></td></tr></table></figure>



<h5 id="3、透射率"><a href="#3、透射率" class="headerlink" title="3、透射率"></a>3、透射率</h5><p>根据已有公式，论文中经过推导得到的透射率计算公式如下：<br>$$<br>t(x) &#x3D; \ 1- \omega \min\limits_c (\min\limits_{y \in \Omega(x)} ( \frac{I^c(y)}{A^c}) )<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">TransmissionEstimate</span>(<span class="params">image, A, size</span>):</span><br><span class="line">    omega = <span class="number">0.95</span> <span class="comment"># 论文里一般采用0.95</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        image[:, :, i] /= A[i]</span><br><span class="line">	</span><br><span class="line">    <span class="comment"># 论文里说明的后者是image图像的暗通道</span></span><br><span class="line">    transmission = <span class="number">1</span> - omega * DarkChannel(image, size)</span><br><span class="line">    <span class="keyword">return</span> transmission</span><br></pre></td></tr></table></figure>

<p>根据论文中的说明，实际上，$\min\limits_c (\min\limits_{y \in \Omega(x)} ( \frac{I^c(y)}{A^c}) )$   就相当于 $ \frac{I^c(y)}{A^c}$  的暗通道图像</p>
<h5 id="4、引导滤波-Guided-Filter"><a href="#4、引导滤波-Guided-Filter" class="headerlink" title="4、引导滤波 (Guided Filter )"></a>4、引导滤波 (Guided Filter )</h5><p>（该部分为优化效果，未在原文中提出，此处的实现主要参考自 <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/36813673%EF%BC%89">https://zhuanlan.zhihu.com/p/36813673）</a></p>
<p>（同时opencv 3.0 中也添加了guided filter的API，可以直接调用）</p>
<p>引导图滤波器是一种自适应权重滤波器，能够在平滑图像的同时起到保持边界的作用</p>
<p>优点：</p>
<ul>
<li>能够克服双边滤波的梯度翻转现象，在滤波后图像的细节上更优</li>
<li>较之传统双边滤波效率高，时间复杂度为O(N)，N是像素个数</li>
</ul>
<p>原论文中的 Guided Filter 的伪代码如下<br>$$<br>1:mean_I &#x3D; f_{mean} (I) \<br>\   \ \ \ \ mean_p &#x3D; f_{mean} (p)\<br>\  \ \  \ \ \ \ \ corr_I &#x3D; f_{mean} (I. *I)\<br>\   \ \ \ \ \ \ \ \ corr_{Ip} &#x3D; f_{mean} (I.*p)\</p>
<p>2:var_I &#x3D; corr_I - mean_I.*mean_I \<br>\ \ \ \ \ \ \ \  \ \ \ \ cov_{Ip} &#x3D; -corr{Ip} - mean_I. *mean_p \</p>
<p>3:a &#x3D; cov_{Ip}.&#x2F;(var_I + \epsilon) \<br>\ \ \ \ \ \ \ \ \  b &#x3D; mean_p - a.*mean_I\</p>
<p>4:mean_a &#x3D; f_{mean}(a)<br>\ \ \ \ \ \ mean_b  &#x3D; f_{mean}(b) \</p>
<p>5:q &#x3D; mean_a. * I + mean_b<br>$$</p>
<p>依照该伪代码在python中使用 opencv 实现如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Guidedfilter</span>(<span class="params">image, p, r, eps</span>):</span><br><span class="line">    mean_I = cv2.boxFilter(image, cv2.CV_64F, (r, r))</span><br><span class="line">    mean_p = cv2.boxFilter(p, cv2.CV_64F, (r, r))</span><br><span class="line">    corr_I = cv2.boxFilter(image * image, cv2.CV_64F, (r, r))</span><br><span class="line">    corr_IP = cv2.boxFilter(image * p, cv2.CV_64F, (r, r))</span><br><span class="line"></span><br><span class="line">    var_I = corr_I - mean_I * mean_I</span><br><span class="line">    cov_Ip = corr_IP - mean_I * mean_p</span><br><span class="line"></span><br><span class="line">    a = cov_Ip / (var_I + eps)</span><br><span class="line">    b = mean_p - a * mean_I</span><br><span class="line"></span><br><span class="line">    mean_a = cv2.boxFilter(a, cv2.CV_64F, (r, r))</span><br><span class="line">    mean_b = cv2.boxFilter(b, cv2.CV_64F, (r, r))</span><br><span class="line"></span><br><span class="line">    q = mean_a * image + mean_b</span><br><span class="line">    <span class="keyword">return</span> q</span><br></pre></td></tr></table></figure>

<p><strong>【未使用引导滤波的结果】</strong></p>
<img src="https://s3.bmp.ovh/imgs/2022/06/15/1990a876e2ed5b82.jpg" style="zoom:80%;" />

<p><strong>【使用引导滤波的结果】</strong></p>
<img src="https://s3.bmp.ovh/imgs/2022/06/15/3bdc4fd9fb6ad79a.jpg" style="zoom:80%;" />





<h5 id="5、去雾之后的结果图像-J-x"><a href="#5、去雾之后的结果图像-J-x" class="headerlink" title="5、去雾之后的结果图像 $J(x)$"></a>5、去雾之后的结果图像 $J(x)$</h5><p>由论文中的公式<br>$$<br>J(x) &#x3D; \frac{I(x)-A}{max(t(x), t_0)} + A<br>$$<br>可得</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">Recover</span>(<span class="params">image, t, A, t0 = <span class="number">0.1</span></span>):</span><br><span class="line">    res = np.empty(image.shape,<span class="string">&#x27;float64&#x27;</span>)</span><br><span class="line">    t = cv2.<span class="built_in">max</span>(t, t0) <span class="comment"># 论文中的t_0 为0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        res[:, :, i] = (image[:, :, i] - A[i]) / t + A[i]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure>

<p>在论文中提到一般给 $t_0$ 的取值为 0.1</p>
<h3 id="三、实验结果及分析"><a href="#三、实验结果及分析" class="headerlink" title="三、实验结果及分析"></a>三、实验结果及分析</h3><p><strong>【实验结果】</strong></p>
<img src="https://s3.bmp.ovh/imgs/2022/06/15/bc1d58fbe362e4c3.jpg" style="zoom:80%;" />

<img src="https://s1.ax1x.com/2022/06/15/XoTLPf.png" alt="XoTLPf.png" style="zoom:80%;" />

<img src="https://s3.bmp.ovh/imgs/2022/06/15/3bdc4fd9fb6ad79a.jpg" style="zoom:80%;" />



<p>当去雾之后，画面整体颜色偏暗，对比度较高</p>
<p>不过我在实验中发现对一下代码进行修改可得到一些不同的结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">TransmissionEstimate</span>(<span class="params">image, A, size</span>):</span><br><span class="line">    omega = <span class="number">0.95</span></span><br><span class="line">    <span class="comment"># image_temp = np.empty(image.shape,&#x27;float64&#x27;)</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="number">3</span>):</span><br><span class="line">        <span class="comment"># image_temp[:, :, i] = image[:, :, i] / A[i]</span></span><br><span class="line">        image /= A[i]</span><br><span class="line">    <span class="comment"># 论文里说明的后者是image图像的暗通道</span></span><br><span class="line">    <span class="comment"># transmission = 1 - omega * DarkChannel(image_temp, size)</span></span><br><span class="line">    transmission = <span class="number">1</span> - omega * DarkChannel(image, size)</span><br><span class="line">    <span class="keyword">return</span> transmission</span><br></pre></td></tr></table></figure>

<img src="https://s1.ax1x.com/2022/06/15/Xo75WT.jpg" alt="Xo75WT.jpg" style="zoom:80%;" />

<p>在计算透射率时直接对原图除以大气光强，可以使得最终结果更加明亮，类似一种高对比度、高曝光的效果</p>
<p>在整个实验中，Guided filter起到了很大的作用，并且一开始为用任何优化实现原文时，得到的结果并不好，对于数据集中的雾霾较为浓厚的图片，整体有一种偏蓝的效果，我猜测应该是图片大多位于室内，再加上人为造成了浓雾弥漫的效果，使得大气光值的计算出现了问题。</p>
<h2 id="Ⅱ、超分辨率算法-Real-ESRGAN-和-Real-CUGAN"><a href="#Ⅱ、超分辨率算法-Real-ESRGAN-和-Real-CUGAN" class="headerlink" title="Ⅱ、超分辨率算法 Real-ESRGAN 和 Real-CUGAN"></a>Ⅱ、超分辨率算法 Real-ESRGAN 和 Real-CUGAN</h2><h3 id="一、相关论文及链接"><a href="#一、相关论文及链接" class="headerlink" title="一、相关论文及链接"></a>一、相关论文及链接</h3><p>腾讯——《Real-ESRGAN: Training Real-World Blind Super-Resolution with Pure Synthetic Data》</p>
<p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2107.10833.pdf%EF%BC%88%E4%B8%8B%E6%96%87%E4%B8%AD%E6%89%80%E4%BB%A5%E5%85%AC%E5%BC%8F%E5%8F%8A%E5%9B%BE%E7%89%87%E5%9D%87%E5%BC%95%E7%94%A8%E8%87%AA%E8%AF%A5%E8%AE%BA%E6%96%87%EF%BC%89">https://arxiv.org/pdf/2107.10833.pdf（下文中所以公式及图片均引用自该论文）</a></p>
<p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/XPixelGroup/BasicSR">https://github.com/XPixelGroup/BasicSR</a></p>
<p>B站——《Real Cascade U-Nets for Anime Image Super Resolution》</p>
<p>项目地址：<a target="_blank" rel="noopener" href="https://github.com/bilibili/ailab/tree/main/Real-CUGAN">https://github.com/bilibili/ailab/tree/main/Real-CUGAN</a></p>
<p>超分重建相关数据集下载地址：<a target="_blank" rel="noopener" href="https://data.vision.ee.ethz.ch/cvl/DIV2K/">https://data.vision.ee.ethz.ch/cvl/DIV2K/</a></p>
<p>Squirrel Anime Enhance下载地址：<a target="_blank" rel="noopener" href="https://github.com/Justin62628/Squirrel-RIFE/releases/tag/v0.0.3">https://github.com/Justin62628/Squirrel-RIFE/releases/tag/v0.0.3</a></p>
<h3 id="二、超分辨率"><a href="#二、超分辨率" class="headerlink" title="二、超分辨率"></a>二、超分辨率</h3><h5 id="1、简介"><a href="#1、简介" class="headerlink" title="1、简介"></a>1、简介</h5><p>超分辨率是计算机视觉的一个经典应用。SR是指通过软件或硬件的方法，从观测到的低分辨率<a target="_blank" rel="noopener" href="https://so.csdn.net/so/search?q=%E5%9B%BE%E5%83%8F%E9%87%8D%E5%BB%BA&spm=1001.2101.3001.7020">图像重建</a>出相应的高分辨率图像（说白了就是提高分辨率），在监控设备、卫星图像遥感、数字高清、显微成像、视频编码通信、视频复原和医学影像等领域都有重要的应用价值。</p>
<p>传统的超分辨率重建技术有基于插值的图像超分辨率和基于重建的图像超分辨率等</p>
<h5 id="2、基于深度学习的图像超分辨率重建技术"><a href="#2、基于深度学习的图像超分辨率重建技术" class="headerlink" title="2、基于深度学习的图像超分辨率重建技术"></a>2、基于深度学习的图像超分辨率重建技术</h5><p>基于深度学习的图像超分辨率重建的研究流程如下：</p>
<ol>
<li><p>首先找到一组原始图像 Image1</p>
</li>
<li><p>然后将这组图片降低分辨率为图像 Image2</p>
</li>
<li><p>通过<strong>各种神经网络结构</strong>，将 Image2 超分辨率重建为 Image3</p>
</li>
<li><p>比较Image1与Image3，验证超分辨率重建的效果，根据效果调节神经网络中的节点模型和参数</p>
</li>
<li><p>反复执行，直到达到较好的效果</p>
</li>
</ol>
<h5 id="3、两种常用的评价超分的指标——PSNR和SSIM"><a href="#3、两种常用的评价超分的指标——PSNR和SSIM" class="headerlink" title="3、两种常用的评价超分的指标——PSNR和SSIM"></a>3、两种常用的评价超分的指标——PSNR和SSIM</h5><p>对超分辨率的质量进行定量评价常用的两个指标是 PSNR (Peak Signal-to-Noise Ratio)和SSIM（Structure Similarity）。这两个值越高代表重建结果的像素值和标准越接近。</p>
<h6 id="PSNR（Peak-Signal-to-Noise-Ratio）峰值信噪比"><a href="#PSNR（Peak-Signal-to-Noise-Ratio）峰值信噪比" class="headerlink" title="PSNR（Peak Signal to Noise Ratio）峰值信噪比"></a>PSNR（Peak Signal to Noise Ratio）峰值信噪比</h6><p>$$<br>MSE &#x3D; \frac{1}{H ×W} \sum \limits_{i &#x3D; 1}^H \sum \limits_{j&#x3D;1}^W(X(i,j)-Y(i,j))^2<br>$$</p>
<p>$$<br>PSNR &#x3D; 10log_{10}(\frac{(2^n-1)^2}{MSE})<br>$$</p>
<p>其中 MSE 表示当前图像 X 与参考图像 Y 的均方误差，H、W分别为图像的高和宽，n为每像素的比特数（比如灰度图的单个像素占8bit）</p>
<p>PSNR 的单位是 db，数值越大表示失真越小</p>
<h6 id="SSIM（Structure-Similarity-）结构相似性"><a href="#SSIM（Structure-Similarity-）结构相似性" class="headerlink" title="SSIM（Structure Similarity ）结构相似性"></a>SSIM（Structure Similarity ）结构相似性</h6><p>$$<br>SSIM(x,y) &#x3D; \frac{(2\mu_x\mu_y + c_1) (2\sigma_{xy} +c_2)}  {(\mu_x^2 +\mu_y^2+c_1) (\sigma_x^2+\sigma_y^2+c_2)}<br>$$</p>
<p>其中 $u$ 为平均值、$\sigma^2$ 为方差，$\sigma_{xy}$ 为协方差 ，$c_1 &#x3D; (k_1L)^2$，$c_2 &#x3D; (k_2L)^2$</p>
<p>是用来维持稳定的常数，$L$ 是像素值的动态范围。$k_1$ &#x3D; 0.01 ,$k_2$ &#x3D; 0.03</p>
<p>结构相似性的范围为-1到1，当两张图像一模一样时，SSIM的值等于1。</p>
<h3 id="三、Real-ESRGAN"><a href="#三、Real-ESRGAN" class="headerlink" title="三、Real-ESRGAN"></a>三、Real-ESRGAN</h3><p>由腾讯ARC实验室发表。在单张图片超分辨率(Single Image Super-resolution)的问题中，许多方法都采用传统的 Bicubic 方法实现降采样，但是这与现实世界的降采样情况不同，太过单一。盲超分辨率(Blind Super-resolution)旨在恢复未知且复杂的退化的低分辨率图像。根据其使用的降采样方式不同，可以分为显式建模(explicit modeling)和隐式建模(implicit modeling)。</p>
<ul>
<li>显式建模：经典的退化模型由模糊、降采样、噪声和 JPEG压缩组成。但是现实世界的降采样模型过于复杂，仅通过这几个方式的简单组合无法达到理想的效果。</li>
<li>隐式建模：依赖于学习数据分布和采用 GAN 来学习退化模型，但是这种方法受限于数据集，无法很好的泛化到数据集之外分布的图像</li>
</ul>
<p><strong>【降级模型】</strong></p>
<p>First-order降级模型如下： </p>
<img src="https://s2.loli.net/2022/07/26/Mp7DkyUAfNn2Bmx.png" alt="image-20220616171654048" style="zoom: 67%;" />

<p>其中，x代表降级后的图像，D代表降级函数，y代表原始图像，k代表模糊核，r代表缩小比例，n代表加入的噪声，JPEG代表进行压缩。</p>
<p>在现实世界中，图像分辨率的退化通常是由多种不同的退化复杂组合而成的。</p>
<p>因此，作者将经典的一阶退化模型(“first-order” degradation model)拓展现实世界的高阶退化建模(“high-order” degradation modeling)，即利用多个重复的退化过程建模，每一个退化过程都是一个经典的退化模型。但是为了平衡简单性和有效性，作者在代码中实际采用的是二阶退化模型(“second-order” degradation model)。流程如下图所示：</p>
<img src="https://s1.ax1x.com/2022/06/15/XTTbt0.png" alt="XTTbt0.png" style="zoom:80%;" />

<ul>
<li>对于模糊核k，本方法使用各项同性（isotropic）和各向异性（anisotropic）的高斯模糊核。</li>
<li>对于Resize操作，常用的方法又双三次插值、双线性插值、区域插值—由于最近邻插值需要考虑对齐问题，所以不予以考虑。在执行缩小操作时，本方法从提到的3种插值方式中随机选择一种。</li>
<li>对于噪声操作，本方法同时加入高斯噪声和服从泊松分布的噪声。同时，根据待超分图像的通道数，加入噪声的操作可以分为对彩色图像添加噪声和对灰度图像添加噪声。</li>
<li>JPEG压缩，本方法通过从[0, 100]范围中选择压缩质量，对图像进行JPEG压缩，其中0表示压缩后的质量最差，100表示压缩后的质量最好。</li>
</ul>
<p>First-order由于使用相对单调的降级方法，其实很难模仿真实世界中的图像低分辨模糊情况。因此，作者提出的High-order其实是为了使用更复杂的降级方法，更好的模拟真实世界中的低分辨模糊情况，从而达到更好的学习效果。</p>
<p>文中提出的高阶降级模型公式如下：<br>$$<br>x &#x3D; D^n(y)&#x3D; (D_n \circ\cdot\cdot\cdot D_2 \circ D_1)(y)<br>$$<br>上式，其实就是对First-order进行多次重复操作，也就是每一个D都是执行一次完整的First-order降级，作者通过实验得出，当执行2次First-order时生成的数据集训练效果最好</p>
<p><strong>【sinc filter】</strong></p>
<p>之后还设置sinc filter来模拟振铃和过冲伪影现象，$sinc$ 滤波器的核如下：<br>$$<br>k(i,j) &#x3D; \frac{\omega_c}{2\pi \sqrt {i^2 + j^2}}J _1(\omega_c \sqrt{i^2 + j^2})<br>$$<br>其中 $(i,j)$ 为核的坐标，$w_c$ 为截止频率，$J_1$ 为一阶的第一种修正<em>Bessel</em>函数。</p>
<img src="https://pic.imgdb.cn/item/62aaf3240947543129f469b7.png" style="zoom:67%;" />

<p>上图为不同截止频率下的振铃和过冲伪影效果</p>
<p>但是因为采用了高阶退化模型，使得退化空间相比于 ESRGAN 来说大得多，训练也就更加具有挑战性。因此作者在 ESTGAN 的基础上做了两个改动：</p>
<ul>
<li>使用 U-Net 判别器替换 ESRGAN 中使用的 VGG 判别器</li>
<li>引入 spectral normalization 来使得训练更加稳定，并减少 artifacts。</li>
</ul>
<p><strong>【生成网络模型】</strong></p>
<p>在网络模型方面，Real-ESRGAN扩展了原来的ESRGAN，同时支持x1,x2,x4。Real-ESRGAN采用与ESRGAN相同的生成网络。对于比例因子×2和×1，使用 pixel-unshuffle 操作（可理解为通过扩大图像通道而对图像尺寸进行压缩），以降低图像分辨率为前提，对图像通道数进行扩充，然后将处理后的图像输入网络进行超分辨重建。</p>
<p><img src="https://pic.imgdb.cn/item/62aaef2e0947543129e9b99c.png"></p>
<p><strong>【对抗网络模型】</strong></p>
<p>由于使用的复杂的构建数据集的方式，所以需要使用更先进的判别器对生成图像进行判别。之前的ESRGAN的判别器更多的集中在图像的整体角度判别真伪，而使用U-Net 判别器可以在像素角度，对单个生成的像素进行真假判断，这能够在保证生成图像整体真实的情况下，注重生成图像细节。</p>
<img src="https://pic.imgdb.cn/item/62aafe5109475431290fb6c0.png" style="zoom:67%;" />

<p><strong>【训练方法】</strong></p>
<ul>
<li>预训练一个以 PSNR 为目标的模型，并采用 L1 loss，得到 Real-ESRNet</li>
<li>用 Real-ESRNet 初始化 Real-ESRGAN 中的 Generator，然后训练 Real-ESRGAN，采用 L1 Loss、perceptual loss 和 GAN loss 三种组合的 loss。</li>
</ul>
<p><strong>【实验结果】</strong></p>
<p><img src="https://pic.imgdb.cn/item/62aafea70947543129109d46.png"></p>
<h3 id="四、Real-ESRGAN-和-Real-CUGAN性能对比"><a href="#四、Real-ESRGAN-和-Real-CUGAN性能对比" class="headerlink" title="四、Real-ESRGAN 和 Real-CUGAN性能对比"></a>四、Real-ESRGAN 和 Real-CUGAN性能对比</h3><p>详细对比（以下表格在 Real-CUGAN 项目中可见）</p>
<table>
<thead>
<tr>
<th></th>
<th>Real-ESRGAN(Anime6B)</th>
<th>Real-CUGAN</th>
</tr>
</thead>
<tbody><tr>
<td>训练集</td>
<td>私有二次元训练集，量级与质量未知</td>
<td>百万级高清二次元patch dataset</td>
</tr>
<tr>
<td>推理耗时(1080P)</td>
<td>2.2x</td>
<td>1x</td>
</tr>
<tr>
<td>效果(见对比图)</td>
<td>锐化强度最大，容易改变画风，线条可能错判，虚化区域可能强行清晰化</td>
<td>更锐利的线条，更好的纹理保留，虚化区域保留</td>
</tr>
<tr>
<td>兼容性</td>
<td>PyTorch支持，VapourSynth支持，NCNN支持</td>
<td>同Waifu2x，结构相同，参数不同，与Waifu2x无缝兼容</td>
</tr>
<tr>
<td>强度调整</td>
<td>不支持</td>
<td>已完成4种降噪程度版本和保守版，未来将支持调节不同去模糊、去JPEG伪影、锐化、降噪强度</td>
</tr>
<tr>
<td>尺度</td>
<td>仅支持4倍</td>
<td>已支持2倍、3倍、4倍，1倍训练中</td>
</tr>
</tbody></table>
<h3 id="五、Squirrel-Anime-Enhance"><a href="#五、Squirrel-Anime-Enhance" class="headerlink" title="五、Squirrel Anime Enhance"></a>五、Squirrel Anime Enhance</h3><img src="https://pic.imgdb.cn/item/62ab0b1509475431292e8fa5.png" style="zoom:67%;" />

<p>Squirrel Anime Enhance是一款基于多个开源超分算法的中文超分软件，具有以下优势：</p>
<ol>
<li>集成了 realCUGAN, realESR, waifu2x 三种超分算法</li>
<li>拥有友好的 GUI 图形界面，方便使用</li>
<li>使用pipe传输视频帧，无需拆帧到本地，拯救硬盘</li>
<li>更小的显存、内存占用，更快的速度</li>
<li>拥有预览界面，能更好地了解超分情况</li>
</ol>
<p>安装较为简单，解压后，双击启动SAE.bat即可启动软件</p>
<p><strong>【使用方式】</strong></p>
<p>在如下界面设置输入的图片或视频文件或其所在文件夹，并设置输出文件夹</p>
<img src="https://pic.imgdb.cn/item/62ab0b2809475431292ebb18.png" style="zoom:67%;" />

 

<p>在如下界面设置超分算法与模型，目前主要集成了realCUGAN, realESR,和waifu2x三种超分算法，每一种超分算法下都有相对应的超分模型。</p>
<img src="https://pic.imgdb.cn/item/62ab0b4509475431292efd9b.png" style="zoom:67%;" />

<p><strong>waifu2x:</strong></p>
<ul>
<li>models cunet：一般用于动漫超分</li>
<li>models photo：一般用于实拍</li>
<li>models style anime：一般用于老动漫</li>
</ul>
<p><strong>realESR:</strong></p>
<ul>
<li>realESRGAN模型，目前支持2x和4x</li>
<li>realESRNet模型：效果较为模糊，细节处不够精密，个人感觉不如realESRGAN模型</li>
</ul>
<p><strong>realCUGAN：</strong></p>
<ul>
<li>up2x-latest-conservative pth</li>
<li>up2x-latest-denoise_2.pth</li>
<li>up3x-latest-consevative.pth </li>
<li>up3x-latest- denoise_3.pth</li>
<li>up4x-latest-conservative pth</li>
<li>up4x-latest-denoise_3.pth</li>
<li>up4x-latest-no denoise.pth</li>
</ul>
<p>其中 Denoise 为降噪版，主要针对较多噪声的情况，conservative为保守版，处理效果较为保守，不会造成严重的失真。No-Denoise为无降噪版，较为通用。</p>
<p>输出分辨率预设：提供倍数及具体像素值的选择</p>
<p>在以上设置完成后，点击一键压制可以开始进行超分操作。</p>
<p><strong>【运行界面】</strong></p>
<img src="https://pic.imgdb.cn/item/62ab0b5709475431292f28e9.png" style="zoom:67%;" />



<h3 id="六、实验结果"><a href="#六、实验结果" class="headerlink" title="六、实验结果"></a>六、实验结果</h3><p>对于Real-ESRGAN 和 Real-CUGAN，下载其在项目地址中公布的release发行版本</p>
<img src="https://s3.bmp.ovh/imgs/2022/06/16/0d60a96c93c8bc93.png" style="zoom: 67%;" />



<img src="https://s3.bmp.ovh/imgs/2022/06/16/7830d22fa7fecc14.png" style="zoom: 67%;" />

<p>通过调用一下命令来运行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">./realesrgan-ncnn-vulkan.exe -i 输入图像.jpg -o 输出图像.png -n 模型名字</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">./realcugan-ncnn-vulkan.exe -i input.jpg -o output.png</span></span><br></pre></td></tr></table></figure>



<p><strong>【结果展示】</strong></p>
<p>[^分辨率说明]: 由于Real-ESRGAN 目前仅支持 X4 ，所以分辨率均使用4倍</p>
<p>原图：分辨率为1600 × 1055</p>
<img src="https://s3.bmp.ovh/imgs/2022/06/16/e8d6883be399c3e7.jpeg" style="zoom:33%;" />



<p>结果图：分辨率为 6400 × 4220</p>
<p>左为 Real-ESRGAN                      右图为 Real-CUGAN</p>
<p><img src="https://pic.imgdb.cn/item/62aaeb4d0947543129de78b0.jpg"></p>
<p>原图：分辨率为1920 × 1080</p>
<img src="https://pic.imgdb.cn/item/62aaeb6b0947543129dec5d2.jpg" style="zoom: 33%;" />



<p>结果图：分辨率为 7680 × 4320</p>
<p>左为 Real-ESRGAN                      右图为 Real-CUGAN</p>
<p><img src="https://pic.imgdb.cn/item/62aaebc00947543129df9a05.jpg"></p>
<p>原图：分辨率为120 × 80</p>
<img src="https://pic.imgdb.cn/item/62aaebd00947543129dfc194.jpg" style="zoom:200%;" />

<p>结果图：分辨率为 7680 × 4320</p>
<p>左为 Real-ESRGAN                      右图为 Real-CUGAN</p>
<p><img src="https://pic.imgdb.cn/item/62aaebdf0947543129dfe26d.jpg"></p>
<p><strong>【结果分析】</strong></p>
<p>本机环境为 Intel(R) Core(TM) i5-9300H CPU @ 2.40GHz  2.40 和  GeForce GTX 1650 ，性能较差 ，无法用于训练。在运行耗时方面，在直接使用exe 文件运行时对于一张标准的 1920 ×1080 图片，Real-ESRGAN 耗时2分钟左右，而Real-CUGAN 仅需四五秒。目前的Real-ESRGAN 仅支持图片4倍放大超分辨率，而Real-CUGAN支持2、3、4倍，使得实验比较均使用×4分辨率进行。总体而言，B站的 Real-CUGAN 效果较好，耗时较短，从我个人的肉眼观察角度出发，腾讯的RealESRGAN纹理保留性较差，较为模糊、细节有所丢失，表现较弱，而B站的 Real-CUGAN对于分辨率较高的图片细节处更为丰富，对于原图较为明亮的图片超分后也可以很好的保持光照亮度。</p>


      

    </section>
    
      <section class='ArticleMeta'>
          <div>
            发布于&nbsp;
            <time datetime="2022-07-27T09:47:18.098Z" itemprop="datePublished">
              2022-07-27
            </time>
          </div>
          
            <div>
              tags: 
  <li class="meta-text">
  { <a href="/tags/CV/">CV</a> }
  </li>


            </div>
          
      </section>
    
    
</article>

  
</div>

            <footer>
    <div>© 2022 - EKKO </div>
    <div>
        <span>
            Powered by <a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a>
        </span>
        ,
        <span>
            Theme - <a target="_blank" rel="noopener" href="https://github.com/nameoverflow/hexo-theme-icalm">Icalm</a>
        </span>
    </div>
</footer>

        </div>
    </div>
</div>

<script src="/js/pager/dist/singlepager.js"></script>

<script>
var sp = new Pager('data-pager-shell')

</script>
</body>
</html>